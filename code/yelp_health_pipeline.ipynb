{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp-AWS Processing Pipeline\n",
    "* Get zip file from Yelp bucket if not in CDH bucket and unzip to json stream - each object is a facility\n",
    "* Process json - extract facility, category, and review datasets\n",
    "* Upload zip, json, and csv's to respective cdh buckets and remove from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import glob\n",
    "import zipfile\n",
    "import gzip\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "from json import JSONDecoder, JSONDecodeError\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import defaultdict\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.glacier.layer1 import Layer1\n",
    "from boto.glacier.concurrent import ConcurrentUploader\n",
    "\n",
    "# Pandas view options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('precision', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program constants\n",
    "BASE_PATH = '/home/ec2-user/yelp/data/'\n",
    "ZIP_PATH = BASE_PATH + 'zip_files/'\n",
    "JSON_PATH = BASE_PATH + 'json_files/'\n",
    "MASTER_PATH = BASE_PATH + 'master_files/'\n",
    "SUMMARY_PATH = BASE_PATH + 'summary_files/'\n",
    "LOG_PATH = BASE_PATH + 'logs/'\n",
    "\n",
    "ZIP_ARCHIVE_NAME = 'yelp_zip_archive_ids.csv'\n",
    "JSON_ARCHIVE_NAME = 'yelp_json_archive_ids.csv'\n",
    "MASTER_ARCHIVE_NAME = 'yelp_master_archive_ids.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDH access parameters\n",
    "CDH_ACCESS_KEY_ID = \"REDACTED\"\n",
    "CDH_SECRET_ACCESS_KEY = \"REDACTED\"\n",
    "CDH_REGION = \"us-east-2\"\n",
    "\n",
    "# Yelp access parameters\n",
    "YELP_ACCESS_KEY_ID = 'REDACTED'\n",
    "YELP_SECRET_ACCESS_KEY = 'REDACTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket names\n",
    "YELP_BUCKET = 'yelp-syndication'\n",
    "CDH_BUCKET_ZIP_GLACIER = 'yelp-zip-files-glacier'\n",
    "CDH_BUCKET_JSON_GLACIER = 'yelp-json-files-glacier'\n",
    "CDH_BUCKET_MASTER_GLACIER = 'yelp-master-files-glacier'\n",
    "CDH_BUCKET_MASTER = 'yelp-master-files'\n",
    "CDH_BUCKET_AUX = 'yelp-auxiliary-files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to s3\n",
    "s3_yelp = boto3.client('s3', aws_access_key_id=YELP_ACCESS_KEY_ID, aws_secret_access_key=YELP_SECRET_ACCESS_KEY)\n",
    "s3_cdh = boto3.client('s3', aws_access_key_id=CDH_ACCESS_KEY_ID, aws_secret_access_key=CDH_SECRET_ACCESS_KEY, region_name=CDH_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get new zip file from Yelp bucket and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_file(file, zip_file_path, json_file_path):\n",
    "    \n",
    "    # Upload files to zip path\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        try:\n",
    "            print('Now downloading:', zip_file_path)\n",
    "            s3_yelp.download_file(Bucket=YELP_BUCKET, Key=file, Filename=zip_file_path)\n",
    "        except:\n",
    "            print('There was a problem uploading the file:',zip_file_path)\n",
    "            return False\n",
    "    else:\n",
    "        print('Zip path does not exist!' 'Please Create the appropriate directories.')\n",
    "        return False\n",
    "\n",
    "    # Unzip files to json path\n",
    "    if os.path.exists(JSON_PATH):\n",
    "        try:\n",
    "            print('Now unziping:', json_file_path)\n",
    "            with gzip.open(zip_file_path, 'rb') as f_in:\n",
    "                with open(json_file_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        except:\n",
    "            print('There was a problem unziping the file:', json_file_path)\n",
    "            return False\n",
    "    else:\n",
    "        print('JSON path does not exist!' 'Please Create the appropriate directories.')\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NOT_WHITESPACE = re.compile(r'[^\\s]')\n",
    "\n",
    "def decode_stacked(document, pos=0, decoder=JSONDecoder()):\n",
    "    while True:\n",
    "        match = NOT_WHITESPACE.search(document, pos)\n",
    "        if not match:\n",
    "            return\n",
    "        pos = match.start()\n",
    "\n",
    "        try:\n",
    "            obj, pos = decoder.raw_decode(document, pos)\n",
    "        except JSONDecodeError:\n",
    "            # do something sensible if there's some error\n",
    "            raise\n",
    "        yield obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    for file in glob.glob(JSON_PATH + '*.json'):\n",
    "        file_name = file.split('/')[-1].split('_')[0]\n",
    "        if os.path.exists(MASTER_PATH) and (MASTER_PATH+file_name+'_facilities.csv') not in glob.glob(MASTER_PATH+'*.csv'):\n",
    "            try:\n",
    "                print('Now reading:', file)\n",
    "                with open(file,'r') as f:\n",
    "                    data = f.read()\n",
    "                print('Now processing:', file)\n",
    "\n",
    "                fac_data = []\n",
    "                cat_data = []\n",
    "                rev_data = []\n",
    "                summary_data = []\n",
    "                count = 0\n",
    "\n",
    "                for fac in decode_stacked(data):\n",
    "\n",
    "                    # Facility data\n",
    "                    fac_id = fac['id']\n",
    "                    fac_name = fac['name']\n",
    "                    fac_is_closed = fac['is_closed']\n",
    "                    review_count = fac['review_count']\n",
    "                    fac_rating = fac['rating']\n",
    "                    fac_updated_time = fac['time_updated']\n",
    "                    phone = fac['phone']\n",
    "                    business_url = fac['business_url']\n",
    "                    fac_url = fac['url']\n",
    "\n",
    "                    # Facility data: location\n",
    "                    address = ' '.join([item for item in fac['location']['address'] if item is not None]).strip()\n",
    "                    city = fac['location']['city']\n",
    "                    state = fac['location']['state']\n",
    "                    country = fac['location']['country']\n",
    "                    postal_code = fac['location']['postal_code']\n",
    "                    latitude = fac['location']['coordinate']['latitude']\n",
    "                    longitude = fac['location']['coordinate']['longitude']\n",
    "\n",
    "                    fac_temp = [fac_id, fac_name, fac_is_closed, review_count, fac_rating, fac_updated_time, phone, business_url, \n",
    "                                fac_url, address, city, state, country, postal_code, latitude, longitude]\n",
    "                    fac_data.append(fac_temp)\n",
    "\n",
    "\n",
    "                    # Categories\n",
    "                    for item in fac['categories']:\n",
    "                        cat_temp = [fac_id, item['alias'], item['title']]\n",
    "                        cat_data.append(cat_temp) \n",
    "\n",
    "\n",
    "                    # Reviews\n",
    "                    for item in fac['reviews']:\n",
    "                        rev_id = item['id']\n",
    "                        rev_rating = item['rating']\n",
    "                        review = item['text']\n",
    "                        user = item['user']['name']\n",
    "                        rev_created_time = item['created']\n",
    "                        rev_url = item['url']\n",
    "                        rev_is_selected = item['is_selected']\n",
    "\n",
    "                        rev_temp = [fac_id, rev_id, rev_rating, review, \n",
    "                                    user, rev_created_time, rev_url, rev_is_selected]\n",
    "                        rev_data.append(rev_temp) \n",
    "\n",
    "                    if count % 100000 == 0:\n",
    "                        print(count, 'facilities processed...')\n",
    "                    count += 1\n",
    "\n",
    "                # Writing csv files...\n",
    "                del data\n",
    "                print('Writing csv files...')\n",
    "\n",
    "                # Write data to csv\n",
    "                fac_cols = ['fac_id','fac_name','fac_is_closed','review_count','fac_rating','fac_updated_time','phone',\n",
    "                            'business_url','fac_url','address','city','state','country','postal_code','latitude','longitude']\n",
    "                fac_data = pd.DataFrame(data=fac_data, columns=fac_cols)\n",
    "                fac_data.to_csv(MASTER_PATH + file_name + '_facilities.csv')\n",
    "                print('Facility data created!')\n",
    "\n",
    "                cat_cols = ['fac_id','alias','title']\n",
    "                cat_data = pd.DataFrame(data=cat_data, columns=cat_cols)\n",
    "                cat_data.to_csv(MASTER_PATH + file_name + '_categories.csv')\n",
    "                print('Category data created!')\n",
    "\n",
    "                rev_cols = ['fac_id', 'rev_id', 'rev_rating', 'review', \n",
    "                            'user', 'rev_created_time', 'rev_url', 'rev_is_selected']\n",
    "                rev_data = pd.DataFrame(data=rev_data, columns=rev_cols)\n",
    "                rev_data.to_csv(MASTER_PATH + file_name + '_reviews.csv')\n",
    "                print('Review data created!')\n",
    "                \n",
    "                # Extract summary data\n",
    "                print('Extracting summary data...')\n",
    "                daily_data = s3_cdh.get_object(Bucket=CDH_BUCKET_AUX, Key='daily_data.csv')\n",
    "                daily_data = pd.read_csv(io.BytesIO(daily_data['Body'].read()), index_col=0)\n",
    "                summary_data.append([file_name, len(cat_data.alias.unique()),\n",
    "                                     len(fac_data), fac_data['fac_rating'].mean(), fac_data['fac_rating'].median(),\n",
    "                                     fac_data['review_count'].mean(), fac_data['review_count'].median(),\n",
    "                                     len(rev_data), rev_data['rev_rating'].mean(), rev_data['rev_rating'].median()])\n",
    "                summary_cols = ['date','cat_count',\n",
    "                                'fac_count','fac_rating_mean','fac_rating_med',\n",
    "                                'fac_rev_count_mean','fac_rev_count_med',\n",
    "                                'rev_count','rev_rating_mean','rev_rating_med']\n",
    "                summary_data = pd.DataFrame(summary_data, columns=summary_cols)\n",
    "                summary_data = summary_data[~summary_data['date'].isin(daily_data['date'])]\n",
    "                daily_data = pd.concat([daily_data,summary_data])\n",
    "                daily_data.to_csv(SUMMARY_PATH + 'daily_data.csv')\n",
    "                daily_data.to_csv(SUMMARY_PATH + file_name + '_daily_data.csv')\n",
    "                print('Summary data created!')\n",
    "                \n",
    "                # Delete temporary data\n",
    "                print('Removing temporary files...')\n",
    "                del fac_data\n",
    "                del cat_data\n",
    "                del rev_data\n",
    "                del daily_data\n",
    "                del summary_data\n",
    "                \n",
    "            except:\n",
    "                print('Data extraction failed! Please retry.')\n",
    "                return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Upload files to s3 and remove from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_aws(local_file, bucket, s3_file, args_dict, overwrite=False):\n",
    "    if 'Contents' in s3_cdh.list_objects(Bucket=bucket).keys() and not overwrite:\n",
    "        uploaded = [item['Key'] for item in s3_cdh.list_objects(Bucket=bucket)['Contents']]\n",
    "    else:\n",
    "        uploaded = []\n",
    "    \n",
    "    if s3_file not in uploaded:\n",
    "        try:\n",
    "            print('Now uploading: ', s3_file)\n",
    "            s3_cdh.upload_file(Filename=local_file, Bucket=bucket, Key=s3_file, ExtraArgs=args_dict)\n",
    "            print(\"Upload Successful\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(\"The file was not found\")\n",
    "            return False\n",
    "        except NoCredentialsError:\n",
    "            print(\"Credentials not available\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files():\n",
    "    try:\n",
    "        # Zip files: glacier\n",
    "        for file in glob.glob(ZIP_PATH + '*.gz'):\n",
    "            local_file_name = file\n",
    "            aws_file_name = file.split(ZIP_PATH)[-1]\n",
    "            bucket_name = CDH_BUCKET_ZIP_GLACIER\n",
    "\n",
    "            args_dict=dict()\n",
    "            args_dict['StorageClass']='GLACIER'\n",
    "\n",
    "            uploaded = upload_to_aws(local_file_name, bucket_name, aws_file_name, args_dict)\n",
    "            if uploaded:\n",
    "                os.remove(file)\n",
    "            else:\n",
    "                print('File ', file, ' not uploaded!')\n",
    "\n",
    "        # JSON files: glacier\n",
    "        for file in glob.glob(JSON_PATH + '*.json'):\n",
    "            local_file_name = file\n",
    "            aws_file_name = file.split(JSON_PATH)[-1]\n",
    "            bucket_name = CDH_BUCKET_JSON_GLACIER\n",
    "\n",
    "            args_dict=dict()\n",
    "            args_dict['StorageClass']='GLACIER'\n",
    "\n",
    "            uploaded = upload_to_aws(local_file_name, bucket_name, aws_file_name, args_dict)\n",
    "            if uploaded:\n",
    "                os.remove(file)\n",
    "            else:\n",
    "                print('File ', file, ' not uploaded!')\n",
    "\n",
    "        # Master files: glacier\n",
    "        for file in glob.glob(MASTER_PATH + '*.csv'):\n",
    "            local_file_name = file\n",
    "            aws_file_name = file.split(MASTER_PATH)[-1]\n",
    "            bucket_name = CDH_BUCKET_MASTER_GLACIER\n",
    "\n",
    "            args_dict=dict()\n",
    "            args_dict['StorageClass']='GLACIER'\n",
    "\n",
    "            uploaded = upload_to_aws(local_file_name, bucket_name, aws_file_name, args_dict)\n",
    "            if uploaded:\n",
    "                print('Glacier upload complete...')\n",
    "            else:\n",
    "                print('File ', file, ' not uploaded!')\n",
    "\n",
    "        # Master files: standard\n",
    "        for file in glob.glob(MASTER_PATH + '*.csv'):\n",
    "            local_file_name = file\n",
    "            aws_file_name = file.split(MASTER_PATH)[-1]\n",
    "            bucket_name = CDH_BUCKET_MASTER\n",
    "\n",
    "            args_dict=dict()\n",
    "            args_dict['StorageClass']='STANDARD'\n",
    "\n",
    "            uploaded = upload_to_aws(local_file_name, bucket_name, aws_file_name, args_dict)\n",
    "            if uploaded:\n",
    "                os.remove(file)\n",
    "            else:\n",
    "                print('File ', file, ' not uploaded!')\n",
    "                \n",
    "        # Summary files: standard\n",
    "        for file in glob.glob(SUMMARY_PATH + '*.csv'):\n",
    "            local_file_name = file\n",
    "            aws_file_name = file.split(SUMMARY_PATH)[-1]\n",
    "            bucket_name = CDH_BUCKET_AUX\n",
    "\n",
    "            args_dict=dict()\n",
    "            args_dict['StorageClass']='STANDARD'\n",
    "\n",
    "            uploaded = upload_to_aws(local_file_name, bucket_name, aws_file_name, args_dict, overwrite=True)\n",
    "            if uploaded:\n",
    "                os.remove(file)\n",
    "            else:\n",
    "                print('File ', file, ' not uploaded!')\n",
    "    \n",
    "    except:\n",
    "        print('File save incomplete! Please retry.')\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files():\n",
    "    # Get file lists\n",
    "    yelp_zip_files = [[item['Key'], item['Key'].split('/')[-1]] for item in s3_yelp.list_objects_v2(Bucket=YELP_BUCKET, Prefix='upenn/')['Contents']]\n",
    "    cdh_zip_files = [item['Key'] for item in s3_cdh.list_objects_v2(Bucket=CDH_BUCKET_ZIP_GLACIER)['Contents']]\n",
    "    \n",
    "    # Process files\n",
    "    for item in yelp_zip_files:\n",
    "        file=item[0]\n",
    "        name=item[1]\n",
    "        zip_file_path = ZIP_PATH+name\n",
    "        json_file_path = (JSON_PATH+name).split('.gz')[0]\n",
    "        \n",
    "        # Check if file has already been processed and archived - upload function MUST verify this\n",
    "        if name not in cdh_zip_files:\n",
    "            # Extract zip and json files\n",
    "            if not get_new_file(file, zip_file_path, json_file_path):\n",
    "                print('There was a problem extracting the zip and json files! Now exiting.')\n",
    "                return False\n",
    "            # Process json file\n",
    "            if not extract_data():\n",
    "                print('There was a problem extracting the data! Now exiting.')\n",
    "                return False\n",
    "            # Upload files to s3\n",
    "            if not save_files():\n",
    "                print('There was a problem saving the files! Now exiting.')\n",
    "                return False\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Program\n",
    "* Add functionality here to shut down EC2 instance when complete and send email notification / add log record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now downloading: /home/ec2-user/yelp/data/zip_files/20200210_businesses.json.gz\n",
      "Now unziping: /home/ec2-user/yelp/data/json_files/20200210_businesses.json\n",
      "Now reading: /home/ec2-user/yelp/data/json_files/20200210_businesses.json\n",
      "Now processing: /home/ec2-user/yelp/data/json_files/20200210_businesses.json\n",
      "0 facilities processed...\n",
      "100000 facilities processed...\n",
      "200000 facilities processed...\n",
      "300000 facilities processed...\n",
      "400000 facilities processed...\n",
      "500000 facilities processed...\n",
      "Writing csv files...\n",
      "Facility data created!\n",
      "Category data created!\n",
      "Review data created!\n",
      "Extracting summary data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:104: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data created!\n",
      "Removing temporary files...\n",
      "Now uploading:  20200210_businesses.json.gz\n",
      "Upload Successful\n",
      "Now uploading:  20200210_businesses.json\n",
      "Upload Successful\n",
      "Now uploading:  20200210_facilities.csv\n",
      "Upload Successful\n",
      "Glacier upload complete...\n",
      "Now uploading:  20200210_categories.csv\n",
      "Upload Successful\n",
      "Glacier upload complete...\n",
      "Now uploading:  20200210_reviews.csv\n",
      "Upload Successful\n",
      "Glacier upload complete...\n",
      "Now uploading:  20200210_facilities.csv\n",
      "Upload Successful\n",
      "Now uploading:  20200210_categories.csv\n",
      "Upload Successful\n",
      "Now uploading:  20200210_reviews.csv\n",
      "Upload Successful\n",
      "File  /home/ec2-user/yelp/data/summary_files/daily_data.csv  not uploaded!\n",
      "Now uploading:  20200210_daily_data.csv\n",
      "Upload Successful\n",
      "All files processed correctly! Our work here is done - See you next time!\n"
     ]
    }
   ],
   "source": [
    "if process_files():\n",
    "    print('All files processed correctly! Our work here is done - See you next time!')\n",
    "else:\n",
    "    print('The program encountered an error! Please check the processing status and restart! =(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
